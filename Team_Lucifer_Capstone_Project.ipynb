{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "yQaldy8SH6Dl",
        "PH-0ReGfmX4f",
        "mDgbUHAGgjLW",
        "Y3lxredqlCYt",
        "2pQ4Wk4Ew6oM",
        "5vbiWx47xFKO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amiya2001/Capstone-Project/blob/main/Team_Lucifer_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "###EDA on Google Play Store Apps and Reviews Data\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -**Mayank Gudadhe\n",
        "##### **Team Member 2 -**Vishal Singh Sangral\n",
        "##### **Team Member 3 -**Richa Rani \n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on exploratory data analysis, this project is created.\n",
        "\n",
        "We will examine Google Play Store data for this project.\n",
        "\n",
        "**Python** is the computer language that we have used to analyse the data.\n",
        "\n",
        "Apps and reviews from the Google Play Store are widely available.\n",
        "\n",
        "They are simple to make and can be profitable. \n",
        "These two reasons have led to an increase in the number of apps being created. \n",
        "\n",
        "In this project, we will compare over 10,000 apps in Google Play from various categories to conduct a thorough research of the Android app industry. \n",
        "\n",
        "In order to develop strategies to promote growth and retention, we will search the data for insights.\n",
        "\n",
        "Let's examine the data, which consists of the following two files:\n",
        "\n",
        "__playstore data.csv__\n",
        "\n",
        "__user reviews.csv__\n",
        "\n",
        "Hey In this project we are going to perform Exploratory Data Analysis , \n",
        "\n",
        "in this we are exploring and analyzing the data to explore the key factor which is responsible for app engagement and success , \n",
        "\n",
        "in this analysis process we use two data set that is given to us one is **Play Store Data.csv** and second one is **User Review.csv**, \n",
        "\n",
        "we are gone through both of it and understand what kind of insight we are taken from it in the first csv file we observe that there is 13 columns and 10841 Rows in it but when we deep die into it and understand each columns properly we observe that many of the columns are object Data type but actually the data in this columns are float and int type , before changing its data type we check for duplicate and null value after removing missing and null value we have to change data type of Reviews , Installs , Price , Last Updated columns and hence our Data cleaning , preprocessing of Play Store Data .csv file is analysis ready after cleaning our row count is 9648 rows and columns count is 13 \n",
        "\n",
        "and now the same operations perform with second csv file in this csv file total 5 columns and 64295 rows in this we observe that there is no such issue of data type but there is maximum null values which is common in 3 data type so our task is to remove that so that our data is clean , so in this csv we find null and missing values and analyze them it is really necessary or not , then our cleaning , preprocessing is completed and now our csv file contains 5 columns and 37427 rows in it , \n",
        "\n",
        "now our both csv files are ready now we are going extract useful insights from this datasets , \n",
        "\n",
        "now we are plotting graph whatever insight we are understand we are plotting it on graph so that it is easily understandable."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/amiya2001/Capstone-Project"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.**Popular Apps category on playstore?\n",
        "\n",
        "**2.**Are the vast majority of apps free or paid?\n",
        "\n",
        "**3.**How essential is the application rating?\n",
        "\n",
        "**4.**What audience segments should the app be based on?\n",
        "\n",
        "**5.**Which classification has the most installations?\n",
        "\n",
        "**6.**What impact does the most recent update have on the rating?\n",
        "\n",
        "**7.**How do ratings change if an app is paid for?\n",
        "\n",
        "**8.**Reviews and ratings: How are they connected?\n",
        "\n",
        "**9.**Let's talk about the subjectivity of sentiment.\n",
        "\n",
        "**10.**Are subjectivity and polarity inversely correlated?\n",
        "\n",
        "**11.**What is the review sentiment percentage?\n",
        "\n",
        "**12.**How does the polarity of emotion differ between premium and free apps?\n",
        "\n",
        "**13.**What impact does content rating have on the app?\n",
        "\n",
        "**14.**Does the date of the most recent update affect the rating?\n",
        "\n",
        "**15.**what are the most used words for all type of sentiment?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Your Business Objective?**"
      ],
      "metadata": {
        "id": "PH-0ReGfmX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Data scientists use exploratory data analysis (EDA) to examine data sets for patterns and abnormalities (outliers), formulate hypotheses based on our understanding of the information, and describe their key characteristics.\n",
        "\n",
        "- Data visualisation techniques are frequently used in exploratory data analysis. In any project involving data analysis or data science, it is a crucial stage.\n",
        " \n",
        "- It assists in deciding how to best modify data sources to obtain the desired results.\n",
        "\n",
        "\n",
        "- In order to better comprehend the data and make it more visually appealing and appealing, EDA entails producing summary statistics for the dataset's numerical data.\n",
        "\n",
        "- The many steps in the EDA process are as follows:\n",
        "\n",
        "    **Problem statement:** We will discuss and analyse the provided data set. We'll examine the traits it has and attempt to conduct a philosophical examination of their significance for this issue.\n",
        "\n",
        "    **Hypothesis:**Studying the attributes in the data base will help us establish some fundamental hypotheses that we can use to play around with the data and see what interesting outcomes we can find.\n",
        "\n",
        "    **Univariate Analysis:** Univariate analysis is the most basic type of data analysis. In order to start, we would choose just one quality and thoroughly examine it. It deals with no co-relation whatsoever, and its primary goal is to describe. Data is taken, summarised, and patterns are discovered within the data.\n",
        "\n",
        "    **Bivariate analysis:** This type of analysis focuses on causality and the connection between two variables. We shall make an effort to comprehend how qualities are interdependent.\n",
        "\n",
        "    **Multivariate Analysis:**When more than two variables must be studied at once, multivariate analysis is used.\n",
        "\n",
        "    **Data Cleaning:**Data cleaning - We'll purge the dataset of errors, outliers, and category variables.\n",
        "\n",
        "    **Testing Hypothesis:** We will determine whether our data complies with the presumptions needed by the majority of multivariate procedures."
      ],
      "metadata": {
        "id": "PhDvGCAqmjP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 20 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing liabraries \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from datetime import datetime as dt\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "#Mount our google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "path='/content/drive/MyDrive/Almabetter /cohart santa/Capstone project 1/'\n",
        "df1=pd.read_csv(path+'Play Store Data.csv')\n",
        "df2=pd.read_csv(path+'User Reviews.csv')"
      ],
      "metadata": {
        "id": "MhCSIhDFsP7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**df1 is Data Frame made by using  Play store data file.**\n",
        "\n",
        "\n",
        "**df2 is Data Frame made by using  User Reviews data file.**"
      ],
      "metadata": {
        "id": "ZeHxsWRVsmMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# This gives first 5 row of our data frame, also here we can cross check weather the data set is correct or not.\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This gives first 5 row of our data frame, also here we can cross check weather the data set is correct or not.\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "pjFnMbawsvyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows_of_df1=len(df1.axes[0])\n",
        "rows_of_df2=len(df2.axes[0])\n",
        "column_of_df1=len(df1.axes[1])\n",
        "column_of_df2=len(df2.axes[1])\n",
        "print(f\"Numbers of Rows in df1 are  {rows_of_df1}\")\n",
        "print(f\"Numbers of column in df1 are  {column_of_df1}\")\n",
        "print()\n",
        "print(f\"Numbers of Rows in df2 are  {rows_of_df2}\")\n",
        "print(f\"Numbers of column in df2 are  {column_of_df2}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "id": "YhQqjft8tXTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Duplicate Value Count in df1\n",
        "df1['App'].value_counts()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate Value Count in df2\n",
        "# user review it is not necessary to find duplicate values there can be many users who review the same app\n",
        "df2['App'].value_counts() "
      ],
      "metadata": {
        "id": "yPnn0ezete8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count of df1\n",
        "df1.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing value of df1\n",
        "plt.rcParams['figure.figsize'] = (15, 12)\n",
        "plt.bar(list(df1.columns),list(df1.isna().sum()) ,color = 'red')\n",
        "plt.title('I AM SHOWING ALL THE NULL VALUES COUNT OF DF1 DATA SET')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Number of Null Values')\n",
        "# we use red color for small values in type and content rating "
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count of df2 \n",
        "df2.isna().sum()"
      ],
      "metadata": {
        "id": "_rXNjmC1tsAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing value of df1\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "plt.bar(list(df2.columns),list(df2.isna().sum()) ,color = 'red')\n",
        "plt.title('I AM SHOWING ALL THE NULL VALUES COUNT OF DF2 DATA SET')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Number of Null Values')"
      ],
      "metadata": {
        "id": "xpOo2Rxhtr9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.** After reviewing the data set I saw that there are 10841 rows and 13 columns in **df1**.\n",
        "\n",
        "**2.** Many of the columns in that dataset* doesn't have the right data types *,\n",
        "the price column has the type 'object' but the column element in that column is numeric type so we have to change that type to int or float.\n",
        "\n",
        "Similarly, there is some column which does not have the right type so we have to change them.\n",
        "\n",
        "**3.** After removing the duplicate value we saw that the total no of rows is 10358 which means we have many duplicate rows in the dataset.\n",
        "\n",
        "**4.** After counting the null/missing value and visualizing the data we saw that there are 5 columns consist null values that are __'Rating'__, __'Type'__,__' Content Rating'__, __'Current ver'__ and __' Android ver'__. \n",
        "\n",
        "Among these 5 column __Rating__ column have the highest number of null values and __Type and Content Rating__ column have the less number of null values.\n",
        "\n",
        "\n",
        "\n",
        "**5.** In **df2**, we saw there are 64295 numbers of rows and 5 columns in the dataset.\n",
        "\n",
        "**6.** The type of all the variables is in the correct form so no need to change that.\n",
        "\n",
        "**7.** In the whole data set, there are 30679 non-duplicated rows present which means there are so many duplicate rows present in the dataset\n",
        "and also after counting the null value and visualizing the dataset,\n",
        "\n",
        "we found that every column have numbers of null values except the 'app' column, \n",
        "\n",
        "if we calculate the percentage then it's almost **42%** of the total rows present in every column."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "list(df1.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df1.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Describe method  will help us find insights about the columns having numerical values. \n",
        "\n",
        "#### Max User Rating is not showing the satisfactory DATA OF 19.00000\n",
        "\n",
        "#### User Rating usually ranges between 1 to 5 star.\n",
        "\n",
        "#### There are huge chances of OUTLIER present in User Rating.\n",
        "\n",
        "#### let just try to visualise this doubt using BOX PLOT.\n"
      ],
      "metadata": {
        "id": "Lh0GaxeCuoa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visulazing the OUTLIER present in DATA \n",
        "df1.boxplot()"
      ],
      "metadata": {
        "id": "pDDSI1dQuuva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### After Visualization our doubt is clear there is an outlier available in USER RATING column , \n",
        "##Hence we need to deal with in DATA WRANGLING PART.\n"
      ],
      "metadata": {
        "id": "nvB4qarKvdUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(df2.columns)"
      ],
      "metadata": {
        "id": "bPqy4VeOvtiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "WMr1GUtcvysv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.boxplot()"
      ],
      "metadata": {
        "id": "JSafyo3Kvyb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first describe the data that the columns, based on our assessment, contain.\n",
        "\n",
        "## Variable description of Playstore_df\n",
        "\n",
        "There are total 13 variable:-\n",
        "\n",
        "play store 13 columns and 10841 rows make up the dataframe. The following list identifies the 13 columns:\n",
        "\n",
        "**App** - It provides a brief description along with information about the application's name .\n",
        "\n",
        "**Category** - It assigns the app a category.\n",
        "\n",
        "**Rating** - This section includes the overall score that each app has gotten from users.\n",
        "\n",
        "**Reviews** - It provides information on the overall number of people who have reviewed the application.\n",
        "\n",
        "**Size** - It informs us of the amount of space taken up by the application on the mobile device.\n",
        "\n",
        "**Installs** – It provides information on the overall quantity of installations and downloads of an application. \n",
        "\n",
        "**Type**-Whether an app is free to use or not is indicated by its Type.\n",
        "\n",
        "**Pricing** - It provides the installation fee for the app. The cost is 0 for apps of the free variety.\n",
        "\n",
        "**Content Rating**-An app's content rating indicates whether it is appropriate for users of all ages or not.\n",
        "\n",
        "**Genres** - This information informs us of the various additional categories to which an application may belong.\n",
        "\n",
        "**Last Updated**-The last time the application was updated is indicated by the last updated date.\n",
        "\n",
        "**Current Ver** - It provides information on the application's most recent version.\n",
        "\n",
        "**Android Ver** - It provides information about the version of Android that can run the application on its platform.\n",
        "\n",
        "## Variable description of userreview_df\n",
        "\n",
        "**App:**- App content App name.\n",
        "\n",
        "**Translated_Review:**- It content review about App\n",
        "\n",
        "**Sentiment:**- Sentiment related to what the emotions users provide while using the app weather it is positive , negative or neutral.\n",
        "\n",
        "**Sentiment_Polarity:**- Sentiment_Polarity refers to the strength of users opinion weather it is strongly positive or negative , polarity lies the range between [-1,1]\n",
        "\n",
        "**Sentiment_Subjectivity:**-Sentiment_Subjectivity lies between the range of [0,1],it is a users personal opinion , judgment it belongs to users which they personally involve in this object.\n",
        "\n",
        "__1.__ After finding the number of columns present in the data set we saw that there are 13 columns in the dataset but when we call describe method it only gives us the description of  one column cause describe method only describes the numeric type column .so we have to change the type of some column like **reviews**, **size**, **install**, **price**, etc.\n",
        "\n",
        "**2.** If we talk about rating then this column has a total of 9367 numbers of non-null values as we saw in the information cell. Mean rating is 4.1, the lowest rating is 1 star and the highest is 19 but we know that in the case of rating, a person can't rate 19 so we have to change this otherwise remove this column for better analysis. \n",
        "\n",
        "**3.** In the case of the user review data frame, there are 5 columns, after calling the _**describe()**_  method we saw that the **'sentiment_polarity'** consists of values in between -1 to 1 and the **'sentiment_subjectivity'** lies between 0 to 1. \n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "QUbdRMEOwcKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Unique Values for each variable in df1"
      ],
      "metadata": {
        "id": "zLWTRAXMtDON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['App'].unique()"
      ],
      "metadata": {
        "id": "j7ZmBUtSj7YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Category'].unique()"
      ],
      "metadata": {
        "id": "b5BKckQlwcKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Rating'].unique()"
      ],
      "metadata": {
        "id": "8US7y2jYcJPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Reviews'].unique()"
      ],
      "metadata": {
        "id": "1Lw8ArI3cYT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Size'].unique()"
      ],
      "metadata": {
        "id": "zqQ2WEokcihx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Installs'].unique()"
      ],
      "metadata": {
        "id": "LxstdRhFhg0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Type'].unique()"
      ],
      "metadata": {
        "id": "nvgWrx8MkO1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Price'].unique()"
      ],
      "metadata": {
        "id": "84zX__eTkPXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Content Rating'].unique()"
      ],
      "metadata": {
        "id": "Ch9Wdvz6kRjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Reviews'].unique()"
      ],
      "metadata": {
        "id": "s4oE_wejkRSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Genres'].unique()"
      ],
      "metadata": {
        "id": "eaEnF7pFkRAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Last Updated'].unique()"
      ],
      "metadata": {
        "id": "n3DxswjckQk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Current Ver'].unique()"
      ],
      "metadata": {
        "id": "JmqqzP-WkP3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Android Ver'].unique()"
      ],
      "metadata": {
        "id": "FUcqHdzqoEId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "id": "UF7KDosTrcAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Unique Values for each variable in df2"
      ],
      "metadata": {
        "id": "Hz4g-f3nrkVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2['App'].unique()"
      ],
      "metadata": {
        "id": "9WWkpRR4rjMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Translated_Review'].unique()"
      ],
      "metadata": {
        "id": "s6O2tcxsrtOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Sentiment'].unique()"
      ],
      "metadata": {
        "id": "PNomBtausHmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Sentiment_Polarity'].unique()"
      ],
      "metadata": {
        "id": "RpW4LVAcsJlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['Sentiment_Subjectivity'].unique()"
      ],
      "metadata": {
        "id": "g8K2Hq7IsI8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.** After calling the __nunique()__ method which gives us the number of unique values present in a particular column, we found that there is a total of 9660 unique apps present in the data frame and it is divided into 40 unique categories .\n",
        "\n",
        "**2.** There is 3 type of unique values present in the  __'Type'__ columnn  but we know that there is only 2 type of apps we use that is paid or free but  we have a extra value present in the type columns , so we have to replace or remove that one. \n",
        "\n",
        "**3.** After checking the unique values in the __'Size'__ column we found that all the value contain _'M'_ and _'K'_ in it __(M for MB and K for KB)__ and also contain a unique value named __'varies with device'__, so in the data cleaning section we have to remove that _'M_' and _'K'_ and convert all the values in __MB__ form and there is no need to change the __'varies with device'__ value.\n",
        "\n",
        "**4.** Like __'Size'__ column  __'Install'__ column has '+'and ',' symbol in it and also the __'Price'__ column has __Dollar__ symbol in it, so for changing the dtype we have to remove the '+' and ',' symbolfrom Installs Column and __'$'__ symbol from __'Price '__Column\n",
        "\n",
        "**5.** In content rating, there are 6 unique values and null values \n",
        "\n",
        "**6.**In the case of the user review data set, there is a total of 1074 unique apps .that means a lot of people give their opinion about these apps only so we can say a lot of people use these apps, and there are 3 types of sentiments that are positive, negative and neutral. \n"
      ],
      "metadata": {
        "id": "-4h5CLPQM2BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "Wp_N3c5CwmGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1) Dealing with OUTLIER in DF1 data **set**"
      ],
      "metadata": {
        "id": "bodZrlenfT97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.describe()"
      ],
      "metadata": {
        "id": "Phe0i3TzgJIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "01LcSmbkCJdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we are finding the elements in df1 have rating more than 5.0\n",
        "df1[df1['Rating'] >5.0]"
      ],
      "metadata": {
        "id": "pK3kWZjTZfYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " In the above data average rating is 19.0, we can deal with this problem by dropoing the element for user rating more than 5.0 \n",
        " we can also see there are null value present in same element in CONTENT RATING COLUMN and ANDROID VERSION COLUMN , "
      ],
      "metadata": {
        "id": "R-49hRzfa5-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2) Hence we need to examine first the NaN values of Content Rating as well as Android Ver columns\n"
      ],
      "metadata": {
        "id": "YXJDXk99bYSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isna().sum()"
      ],
      "metadata": {
        "id": "VHG4EqbeCVgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the null values of Android Version Column\n",
        "df1[df1['Android Ver'].isnull()]"
      ],
      "metadata": {
        "id": "fYlHR_5eAJDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the null values of Content Rating column\n",
        "df1[df1['Content Rating'].isnull()]"
      ],
      "metadata": {
        "id": "6ogM99x5Atxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining both the columns(Content Rating and Android ver) and the outlier present in Rating column we can move forward with following steps:\n",
        "\n",
        "1) Eliminate the outlier first and than look for value to replace in android ver and Content Rating or drop the element from the data frame if we cannot replace it .\n",
        "\n",
        "2) If elimination or dropping an element is a solution to deal with data then it strongly recommended that we try to drop elements from Android version first because we can deal with three problem at a time while handling the NaN values of Android version,"
      ],
      "metadata": {
        "id": "TZu0VIM0BJiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Examining the different values of Android Version which may replace the NaN Values of the columns\n",
        "df1['Android Ver'].value_counts()"
      ],
      "metadata": {
        "id": "CUAvf_cnbCnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are too many different values avaialble in Android Version , we cannot identify the perfect value for the elements containing NaN values to replace and there are only 3 elements which contains NULL VALUES which will not effect the data very much.\n",
        "\n",
        " **SO THESE ELEMENTS CAN BE DROPPED**"
      ],
      "metadata": {
        "id": "D7kfB_LKD7FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(index = [4453,4490,10472], inplace = True)"
      ],
      "metadata": {
        "id": "kw8g1q_FDzjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recheck the shape as raw data shape is \n",
        "df1.shape"
      ],
      "metadata": {
        "id": "h0IsiIrXbedm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "x9KyloQebfy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Examine again the null values of df1\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "RZn3eBsYbsfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see here ,\n",
        "\n",
        "We handle Android Version Null Values which results in handling Content rating null values also handelling the outlier value in Rating more than 5 star in Rating columns ."
      ],
      "metadata": {
        "id": "Zfg11aChiur-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3) Examine and handling the NaN Values of Current Version:"
      ],
      "metadata": {
        "id": "xdMxPefpWo75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#rows containing null values in Current Version\n",
        "df1[df1['Current Ver'].isnull()]"
      ],
      "metadata": {
        "id": "kjzuTOmeVrlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Examining the different values of Current Version which may replace the NaN Values of the columns\n",
        "df1['Current Ver'].value_counts()"
      ],
      "metadata": {
        "id": "UxuZ905kXj_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no particular value to replace in current version and there are only 8 rows which we deal droppig them from data frame."
      ],
      "metadata": {
        "id": "Qd_zaDFIjGJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "id": "N6y47Dl5kjNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DROPPING NULL VALUES OF CURRENT VERSION \n",
        "df1=df1[~df1['Current Ver'].isnull()]"
      ],
      "metadata": {
        "id": "e3mv0gmKYbwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "id": "UgqS0R_gk_hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "bYLK3OLQlCSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4) Examine and handling the NaN Values of Type:"
      ],
      "metadata": {
        "id": "FGRrzpPJR-MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1[df1['Type'].isnull()]"
      ],
      "metadata": {
        "id": "LgVzjqpxlS9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the type fo data and its count\n",
        "df1['Type'].value_counts()"
      ],
      "metadata": {
        "id": "oTRl5aL2meBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing the null values with FREE\n",
        "df1.loc[9148,'Type']='Free'"
      ],
      "metadata": {
        "id": "xSyJ-wnDqDKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5) Examine and handling the NaN Values of Rating:"
      ],
      "metadata": {
        "id": "dt2Q4tRyl-ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the null counts of df1 after handling null values of Type column \n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "pltCCQa4qTWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[df1.Rating.isnull()]"
      ],
      "metadata": {
        "id": "CFebbqntSOVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization of distribution of rating using displot and detecting the outliers through boxplot.\n",
        "fig, ax = plt.subplots(2,1, figsize = (12,7))\n",
        "sns.distplot(df1['Rating'],ax = ax[0])\n",
        "sns.boxplot(x='Rating',data=df1,ax = ax[1])"
      ],
      "metadata": {
        "id": "pO2375_cmgxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lWauDwLa2Bqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating mean and median of Rating column of not null value to replace the null values\n",
        "median_of_rating=df1[~df1.Rating.isnull()].Rating.median()\n",
        "print(f'median_of_rating = {median_of_rating}')\n",
        "mean_of_rating=round(df1[~df1.Rating.isnull()].Rating.mean(),1)\n",
        "print(f'mean_of_rating = {mean_of_rating}')"
      ],
      "metadata": {
        "id": "Alp7zzbjmdaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing the null values with median of rating \n",
        "df1['Rating'].fillna(value= median_of_rating,inplace=True)"
      ],
      "metadata": {
        "id": "mLDHVpfJvj4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining null value count\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "4wzzUHfQv5KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6) Correction of DATA TYPE OF columns in DF1:"
      ],
      "metadata": {
        "id": "Bado7KinWAjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "iAlve517wAqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the type of 'Reviews' column from object to int64\n",
        "df1=df1.astype({'Reviews':int})\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "-LG84-3sBaiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for removing 'M'  and 'k' \n",
        "def remove_k_m(size):\n",
        "  \n",
        "    if 'M' in size:\n",
        "      size = size[:-1]\n",
        "      return size\n",
        "\n",
        "    elif 'k' in size:\n",
        "      size = round(float(size[:-1])/1024,4)\n",
        "      return size\n",
        "    \n",
        "    else:\n",
        "      return size"
      ],
      "metadata": {
        "id": "fcVOazBSB5SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing 'M' and 'k' from Size column\n",
        "df1.Size = df1.Size.apply(lambda x : remove_k_m(x))"
      ],
      "metadata": {
        "id": "SOniwax0D7Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verifying Size column\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "ZmRMSRXcb_Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Size.unique()"
      ],
      "metadata": {
        "id": "HlKTGnT4FbST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Installs'].value_counts"
      ],
      "metadata": {
        "id": "wT8yoxXQcM8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for removing '+' and ',' from install column\n",
        "def replace(char):\n",
        "  new_char=char\n",
        "  count=char.count(',')\n",
        "\n",
        "  if ',' in char:\n",
        "    new_char=char.replace(',','')\n",
        "\n",
        "  if '+' in new_char:\n",
        "    new_char=int(new_char[:-1])\n",
        "    return new_char\n",
        "    \n",
        "  else:\n",
        "    return int(new_char)"
      ],
      "metadata": {
        "id": "z0aIbI80D9VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replace the ',' and '+' and change the dtype\n",
        "df1.Installs =df1.Installs.apply(lambda x:replace(x))"
      ],
      "metadata": {
        "id": "EjzqYFUUIQZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "Tc4S9acaVvoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reviewing the price column which have paid type\n",
        "df1[df1['Price'] !='0']"
      ],
      "metadata": {
        "id": "jitBSO2zdI2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for removing dolar sign\n",
        "def remove_dollar_sign(price):\n",
        "  new_price=price\n",
        "\n",
        "  if '$' in price:\n",
        "    \n",
        "    new_price = price.replace('$','')\n",
        "    return float(new_price)\n",
        "  \n",
        "  else:\n",
        "    return float(new_price)"
      ],
      "metadata": {
        "id": "doXT0HHUdXv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the dollar and changing the dtype of price column\n",
        "df1.Price = df1.Price.apply(lambda x : remove_dollar_sign(x))"
      ],
      "metadata": {
        "id": "d30E-9ogdjoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#crosschecking weather dollar sign remove or not\n",
        "df1['Price'].dtype"
      ],
      "metadata": {
        "id": "unj6gZXzd22W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the formate of LAST UPDATING COLUMN\n",
        "df1['Last Updated'].head()"
      ],
      "metadata": {
        "id": "VxaP3yRDf8ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#coverting the data type of 'Last Update' column into date time\n",
        "df1['Last Updated'] = df1['Last Updated'].apply(lambda x: dt.strptime(x,'%B %d, %Y'))"
      ],
      "metadata": {
        "id": "cf1xJZhFgJkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Last Updated'].head()"
      ],
      "metadata": {
        "id": "ZCUFAdRZGH3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "Gz-lpy4Wg319"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7) Dealing with duplicate values :\n",
        "\n"
      ],
      "metadata": {
        "id": "_PuC7gQQhIV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Examining the duplicate values in df1 APP columns\n",
        "df1['App'].value_counts()"
      ],
      "metadata": {
        "id": "aEokHlMthIFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop the duplicate values\n",
        "df1.drop_duplicates('App',inplace=True)"
      ],
      "metadata": {
        "id": "VX1tkcnDjTjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross verifying if there is any duplicate values in APP columns\n",
        "df1['App'].value_counts()"
      ],
      "metadata": {
        "id": "Mi_osEj2jd31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here DF1 Data looks cleaned and refined\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "4e22d4ybjzqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.describe()"
      ],
      "metadata": {
        "id": "UyoCraMBkJtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8) Working on User review Data which df2"
      ],
      "metadata": {
        "id": "3FCiihhgkWZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "id": "m46dxMOzkvGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Examining the null value counts in DF2\n",
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "pWTetlywlN89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the null values in 'Translated_Review ' column\n",
        "df2[df2.Translated_Review.isnull()]"
      ],
      "metadata": {
        "id": "Mt14zGxzlb79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the null values in '\tSentiment ' column\n",
        "df2[df2.Sentiment.isnull()]"
      ],
      "metadata": {
        "id": "0vTR37R0lqGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the rows which have null value in '\tTranslated_Review' and non null value in  'Sentiment'\n",
        "df2[df2.Translated_Review.isnull() & ~df2['Sentiment'].isnull()]"
      ],
      "metadata": {
        "id": "mA7CJUmSlvoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the rows which have null value in '\tSentiment' and non null value in  'Translated_Review'\n",
        "df2[df2.Sentiment.isnull() & ~df2['Translated_Review'].isnull()]"
      ],
      "metadata": {
        "id": "NgQYv_CamAzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping all the null values in the data\n",
        "df2.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "9RGAnXy5neP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "id": "SQYb5xUHnhIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Re- examining the null values \n",
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "m8Rc57LqnnOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.describe()"
      ],
      "metadata": {
        "id": "Rjn2dg-unqdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "8JM2SfibwmG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ### __For Play Store Data__\n",
        "  **1.** So in this data wrangling section at first we clean and do some \n",
        "  changes in the play store dataset and after that we do the same with \n",
        "  user review dataset.\n",
        "\n",
        "  **2.** In the case of the play store Dataset,first we deal with the null values \n",
        "  present in every column.\n",
        "\n",
        "  **3.** In the previous, we use the ___isnull()___ and ___sum()___\n",
        "  method to know exactly how many null values present in every column \n",
        "  and we found that there is only 1 null value present in both __'Type'__ \n",
        "  and __'Content Rating'__ column,8 null values present in the __'Current Ver'__ \n",
        "  column,3 null value is in the __'Android Ver'__ column and the highest no of null values (that is 1474) are in the __'Rating'__ column.\n",
        "\n",
        "  **4.** Firstly we deal with the columns having a lesser number of null values\n",
        "  that is __'Type'__ and __'Content Rating'__, so in the Type column we saw that there  is 3 unique values (that is free, paid, and 0), there are 10039 numbers of 'Free'  type,800 number 'Paid' type and only one '0' type but we know that zero is not a type and if we analyze that column only, then we also saw that \n",
        "  in the same row, the __'Rating'__ column has '19.0' as a value, and we know \n",
        "  that is not the right value that means this row is an outlier and also \n",
        "  both __'Content Rating'__ and __'Android Ver'__ have null values so we have to \n",
        "  remove this row permanently.\n",
        "\n",
        "  **5.** So for the type column, the null value will replace with the value 'Free' cause the most number of apps are 'Free' type.  \n",
        "\n",
        "  **6.**  After the __'Type'__ column, we have to deal with the Android ver column, and as we know previously that there are only 3 null values present in the Android Ver column.\n",
        "\n",
        "  **7.** After deeply analyzing that column we find that we can't replace the null values with the ___mean___ or ___median___, and also the outlier row which we find when we analyze the __'Type'__ column also present there so we have to remove all these 3 columns from the dataset.\n",
        "  **8.**  we use the pandas ___drop()___ method for removing these columns and set ___'iplace=True'___ for permanent changes in the data frame.\n",
        "\n",
        "  **9.** Now if we call ___isnull()___ and ___sum()___ methods we can see that only the __'Rating'__ and __'Current Ver'__ columns have null values.\n",
        "  now we have to deal with these 2 columns.\n",
        "\n",
        "  **10.** In the case of the __'Current Ver'__ column we have to remove all the rows containing null values cause like the __'Android Ver'__ column we can't replace the null value of __'Current Ver'__ with the mean or median.\n",
        "\n",
        "  **11.** After dropping all null values we have only one column which contains the null values that are in The __'Rating'__ column. As we can see there are 1470 numbers  of null values in this column, so we can't just remove these rows cause it's a very huge amount of data. so we have to replace these data with the mean of the column.\n",
        " **12.** Without including NaN values, the mean of __'Ratings'__ column is calculated to be 4.2.\n",
        "\n",
        " **13.**The median of the items in the \"Rating\" column  is 4.3. This indicates that 50% of the apps have an average rating above 4.3 and the remaining 50% have a rating below 4.3.\n",
        "\n",
        "\n",
        " **15.**The ratings are left biassed, as seen in the distplot representations.\n",
        "We are aware that if a variable is skewed, the values at the extreme ends of the distribution will affect the mean. As a result, the median provides a more accurate depiction of the vast majority of values in the variable.\n",
        "\n",
        " **16.** So, we will use its median to impute the NaN values in the Rating column.\n",
        "\n",
        "  **17.** For finding the median of the column we use the **median()** function with nonnull values of that column.After finding the median we replace all the null values with the median.\n",
        "\n",
        "  **18.** After these operations we can see there is no null value present in the data frame and we got 10830 rows and 13 columns.\n",
        "\n",
        "  **19.** After null values, we have to work on the ___dtype___ of all columns, so as we have seen previously that there are some columns (like Reviews, Size, Installs, Price, and Last updated) which needs some changes.\n",
        "\n",
        "  **20.** Firstly for the __'Reviews'__ column, we know that it contains numeric type data so we have to change the ___dtype___ of this column .for changing _dtype_ we use ___astype()___ method. \n",
        "\n",
        "  **21.** After the __'Reviews'__ column, we find that the __'Size'__ column consists of data in __mb__ and __kb__ format __(m for mb and k for kb)__ so we have to remove the __'m'__ and __'k'__ sign from the data and convert the data to mb if it is in kb. for that we write a function that takes the data of the __'Size'__ column as an argument and returns the data without __'m'__ and __'k'__ signs.\n",
        "\n",
        "  **22.** After writing the function we pass the __'Size'__ column \n",
        "  inside the __lambda__ function with the help of ___apply()___ method and stored the new\n",
        "  value in place of old values.\n",
        "\n",
        "  **23.** After this operation, if we call the __head()__ method \n",
        "  and we can see that the __'Size'__ column contains the new value without __'m'__ and __'k'__ sign and all the kb data is converted into mb.\n",
        "\n",
        "  **24.** Like size column __'Install'__ column have some \n",
        "  special symbols like __'+'__ and __','__, so for removing that symbol and converting the __dtype__ we write a function named ___replace___ which takes the data of the __'Install'__ column as an argument and returns the same data without __'+'__ and __','__ symbol and __int__ \n",
        "  type. like __'Size'__, we pass the __'Install'__ column inside the __lambda__ function using ___apply()___ method and got the column updated .\n",
        "\n",
        "  **25.** Now if we call the ___info()___ method we can see \n",
        "  that the values in the __'Install'__ column got updated and the ___dtype___ is also changed.\n",
        "\n",
        "  **26.** After the __'Size'__ and __'Install'__ column it's time for the __'Price'__ column, so for the __'Price'__ column ,first we check how many rows there ,which have some price, and then we found\n",
        "  that there is __797__ row that has some price, not __0__, and also one more thing we found that all these data contain a __Dollar__ symbol in it so we have to remove the __Dollar__ symbol for changing the ___dtype___. for that, we write  a function named ___'remove_dollar'___ which takes the values of the __'Price'__ column as an argument and return the same value with __float__ type and without ___'$'___ symbol.\n",
        "\n",
        "  **27.** Like __'Size'__ and __'Install'__ we use ___apply ()___ method and __lambda__ function to do this operation.\n",
        "\n",
        "  **28.**  At last, We changed the type of the __'Last Updated'__ column from object to \n",
        "  datetime by using ___datetime.strptime ()___ function inside the __lambda__ function and use ___apply()___ method on it.\n",
        "\n",
        "  **29.**Now if we call the ___info()___ method then we can see that all the columns mention above is updated with its ___dtype___ and values\n",
        "\n",
        "### __For User Review Data__\n",
        "    \n",
        "**1.** In the case of the ___user review___ data set, we know that there are many null values present in every column except the __'App'__ column.\n",
        "\n",
        "**2.** After checking the null values we saw that if the __'Translated_Review'__ column has a null value then the __'Sentiment'__,\n",
        "__'Sentiment_Polarity'__ and __'Sentiment_subjectivity'__ columns also have null values\n",
        "\n",
        "**3.**This happens because of the unavailability of reviews, If a person did not give any review then it's impossible to find the __'Sentiment'__, __'Sentiment_polarity'__\n",
        "and __'Sentiment_subjectivity'__, but we found some exception data in the data frame which didn't follow this rule so we have to delete all the rows consist null values because we don't want that rows which didn't have any review.\n",
        "\n",
        "**4.**After deletingthe null values there are only ___37427___ data present in our data frame.  \n",
        "    Answer Here."
      ],
      "metadata": {
        "id": "TOJ7YE4mwmG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "NT5fmaaewv22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Popular app category on play store"
      ],
      "metadata": {
        "id": "UTZ4_ClLwv23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top catagory containing apps\n",
        "figure = plt.figure(figsize=(20,15))\n",
        "s =df1['Category'].value_counts()\n",
        "yaxis = list(s.index)\n",
        "xaxis = list(s)\n",
        "plt.barh(yaxis,xaxis)\n",
        "plt.title('Categories containing numbers of apps')\n",
        "plt.ylabel('Category')\n",
        "plt.xlabel('Number of Apps')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IJkbuhlowv23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QLoyJ5eJwv24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pick this chat to know what are the top category containing the highest number of App."
      ],
      "metadata": {
        "id": "yNKiur00wv24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "cNjuMKkWwv25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graph, we found that the top five categories which have the highest no Apps are Family, Games, Tools, Business, and medical sector\n"
      ],
      "metadata": {
        "id": "LnR0guCWwv25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "jgIX0wiJwv25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes, the insights we found created a positive business impact.\n",
        "\n",
        "- looking at the above graph we can say that in the google play store, there are the highest number of  apps in the family category and the lowest number of apps in the beauty category "
      ],
      "metadata": {
        "id": "wcm-Jx_Pwv26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "7z_SsdKPwv26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the avg rating of apps\n",
        "df1.Rating.value_counts().plot.bar(color='red',figsize=(15,8))\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('AVG')\n",
        "plt.title(\"visualize the avg rating of apps\")"
      ],
      "metadata": {
        "id": "xXWHuzM0wv26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#) checking Last Update date has an effects on rating\n",
        "#fetch update year from date\n",
        "df1[\"Update year\"] = df1[\"Last Updated\"].apply(lambda x: x.strftime('%Y')).astype('int64') "
      ],
      "metadata": {
        "id": "Yl9HjqV2oyPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "sns.regplot(x=\"Update year\", y=\"Rating\", data=df1)\n",
        "plt.title(\"Update Year VS Rating\")"
      ],
      "metadata": {
        "id": "oea97VBBo5jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ibtfitWmwv27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows us a number of ratings for apps present in the play store Dataframe."
      ],
      "metadata": {
        "id": "If5KXmYSwv28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "6DWVQvmUwv28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this graph we found that many people give 4.3 and 4.4 ratings to many apps and very less people give 1.2,1.5 and 1.4 rating, Here 4.2 can't consider cause it is the mean which we replace with null values."
      ],
      "metadata": {
        "id": "SprsDJgDwv2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Xxq1X8rYwv3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that most of the apps have got an average rating of 4.3 that means most of the apps have a good rating."
      ],
      "metadata": {
        "id": "TVRSouvEwv3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "b7w6M3lGwv3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize how many free and paid version are there\n",
        "df1.Type.value_counts().plot.pie(figsize=(6,12),autopct='%1.1f%%')\n",
        "plt.legend(loc='upper right')"
      ],
      "metadata": {
        "id": "8lGhG_aNwv3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "NOmrM75owv3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pie chart gives us how many free and paid types of apps are there"
      ],
      "metadata": {
        "id": "ntp814tRwv3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "9rI5U6hIwv3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we found in the play store  data set that 92.2% of apps are free type and the other 7.8% are paid type\n"
      ],
      "metadata": {
        "id": "4JFSM8O9wv3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-dO_rdqMwv3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zUGlq8Dowv3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "kWXX0U3Pwv3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing the content rating columns\n",
        "df1['Content Rating'].value_counts().plot.pie(figsize=(7,12),wedgeprops={'width':0.65},autopct='%1.1f%%')\n",
        "plt.legend(bbox_to_anchor=(1.3,1),loc='upper right')\n"
      ],
      "metadata": {
        "id": "mk9jm-i5wv3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ymM6fAkuwv3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we pick this Donut chart to visualize the __'Content Rating'__ column\n"
      ],
      "metadata": {
        "id": "x8mH6L5iwv3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "sXo7LEO9wv3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, We can clearly see majority of apps can be used by everyone"
      ],
      "metadata": {
        "id": "_0MmBSFwwv3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "JN93QWHzwv3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "s=df1.groupby('Category').Installs.mean()\n",
        "figure=plt.figure(figsize=(17,10))\n",
        "plt.barh(s.index,s,color='maroon')\n",
        "plt.xlabel('AVG Installs')\n",
        "plt.ylabel('Category')\n",
        "plt.title(\"visualize the highest AVG Install \")"
      ],
      "metadata": {
        "id": "5_vqkqV9wv3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "VrsAdVN4wv3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we pick this graph to know which category have highest Average number of Installs."
      ],
      "metadata": {
        "id": "lGewPbutwv3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "RVnGHWYAwv3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows us that ThevCommunication,video_playerand social  categories has the highest number of installs.\n",
        "\n"
      ],
      "metadata": {
        "id": "swHpzGs6wv3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "N1bqFfuawv3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "df=df1.sort_values(by=['Installs'],ascending=False)\n",
        "df[df.Installs==df.Installs.max()]"
      ],
      "metadata": {
        "id": "IuOwEDL5wv3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.Installs==df.Installs.max()].groupby('Category').Type.value_counts().plot.bar(figsize=(10,5))\n",
        "plt.ylabel('No. of Installs')\n",
        "plt.title(\"visualize the Top 20 apps According to Installs\")"
      ],
      "metadata": {
        "id": "PPAuETpJ5atn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Z3UiPDekwv3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the top 20 apps are from which category and which type"
      ],
      "metadata": {
        "id": "BDclg21Vwv3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "Z7ychVHpwv3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the number of installs the top 20 apps are free type and many of them are from Communication category"
      ],
      "metadata": {
        "id": "gtSsw7S2wv3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "kzcWxN7wwv3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "type_paid= df1[df1.Type=='Paid']\n",
        "type_paid.shape"
      ],
      "metadata": {
        "id": "apmOgZxVwv3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_paid.groupby('Price')['App'].count().plot.bar(figsize=(20,8))\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('No of Apps')\n",
        "plt.title(\"visualize the no.of apps Acc. to Price\")"
      ],
      "metadata": {
        "id": "5dff-18d7Oj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "lNWooijLwv3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the highest number of apps present in which price range\n"
      ],
      "metadata": {
        "id": "IJA2DT19wv3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "-JPs8if0wv3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above Plot we conclude that from paid type apps highest number of apps are usd 0.99 and usd 2.99"
      ],
      "metadata": {
        "id": "SocTdb0cwv3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "yppy5l8vwv3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "type_paid['Revenue']=type_paid.Installs*type_paid.Price\n",
        "type_paid.groupby('Category')['Revenue'].mean().sort_values(ascending= False).plot.bar(figsize=(18,8),color='r')\n",
        "plt.ylabel('AVG Revenue')"
      ],
      "metadata": {
        "id": "tPJKEwHNwv3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ryD1SdPgwv3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the revenue by category, we choose this graph."
      ],
      "metadata": {
        "id": "ymtP30lZwv3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "5SCnFqygwv3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the above graph, the lifestyle sector generates the most amount of money, while the social sector generates the least amount of revenue."
      ],
      "metadata": {
        "id": "zC4IPNUfwv3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "2Df6_p0Kwv3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "type_paid.groupby('Category').Price.mean().sort_values(ascending= False).plot.barh(figsize=(10,10)).invert_yaxis()"
      ],
      "metadata": {
        "id": "wPe1X_y8wv3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.groupby('App')[['Reviews','App']].sum().nlargest(20,['Reviews']).plot.barh(figsize = (12,7), color = 'blue').invert_yaxis()"
      ],
      "metadata": {
        "id": "6F5vcT718mmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "5X2rrWuNwv3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LFW-z6c_wv3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GVgB6uAUwv3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BCevMxyvwv3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "CyEbDEBMwv3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mRphziXCwv3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "f5lFe234wv3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Function to group the apps based on its size in MB\n",
        "\n",
        "def group_size(size):\n",
        "  \n",
        " \n",
        "  try:\n",
        "    size=float(size)\n",
        "    if size < 1:\n",
        "      return 'Below 1'\n",
        "    elif size >= 1 and size <10:\n",
        "      return '1-10'\n",
        "    elif size >= 10 and size <20:\n",
        "      return '10-20'\n",
        "    elif size >= 20 and size <30:\n",
        "      return '20-30'\n",
        "    elif size >= 30 and size <40:\n",
        "      return '30-40'\n",
        "    elif size >= 40 and size <50:\n",
        "      return '40-50'\n",
        "    elif size >= 50 and size <60:\n",
        "      return '50-60'\n",
        "    elif size >= 60 and size <70:\n",
        "      return '60-70'\n",
        "    elif size >= 70 and size <80:\n",
        "      return '70-80'\n",
        "    elif size >= 80 and size <90:\n",
        "      return '80-90'\n",
        "    else:\n",
        "      return '90 and above'\n",
        "  except:\n",
        "    return size\n",
        "#userreview_df.info()"
      ],
      "metadata": {
        "id": "XUnacG2Awv3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Groupsize'] = df1.Size.apply(lambda x: group_size(x))\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "acWyE9UU85cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1.Groupsize.value_counts().plot.pie(figsize=(8,10),wedgeprops={'width':0.5},autopct='%1.1f%%')\n",
        "plt.legend(bbox_to_anchor=(1.3,1) ,loc='upper right')"
      ],
      "metadata": {
        "id": "9AtflhZp9A73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Rc070e73wv3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this graph shows the percentage of apps present in different groups which we \n",
        "create on the above cell"
      ],
      "metadata": {
        "id": "azeNqS7pwv3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "sHWFdLJKwv3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*With* the exception of the group \"varies with device,\" we discovered from the above graph that larger apps take up less space than smaller ones."
      ],
      "metadata": {
        "id": "Z1mppGlHwv3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "tQ6jOYg6wv3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "8Q7nNCYlwv3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "FLVfMCltwv3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.groupby('Groupsize').Reviews.mean().plot.barh(figsize=(15,7))\n"
      ],
      "metadata": {
        "id": "xikltxGVwv3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.groupby('Groupsize').Installs.mean().sort_values(ascending= False).plot.barh(figsize=(15,7))\n",
        "plt.title(\"Average number of app installs \")\n",
        "plt.ylabel('App size in MB')\n",
        "plt.xlabel('Average no of app installs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5uUD7FWl9iSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "80u7DF-bwv3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we pick this graph to know which size group has highest number of installs and reviews."
      ],
      "metadata": {
        "id": "MvTyJyBEwv3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "uVDV_KWZwv3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the exception of the fact that we can conclude that the app with a larger size has more reviews and installs, the preceding graph clearly shows that the group with the greatest average number of reviews and installs is \"varies with device.\""
      ],
      "metadata": {
        "id": "pJ5b5Hydwv3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "pYfxTTniwv3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "JF1CKo0pwv3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "sV6TaeiRwv3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# percentage of all reviews\n",
        "df2.Sentiment.value_counts().plot.pie(figsize=(8,10),wedgeprops={'width':0.5},autopct='%1.1f%%')\n",
        "plt.legend(bbox_to_anchor=(1.3,1) ,loc='upper right')"
      ],
      "metadata": {
        "id": "V_uDgpYlwv3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apps with the highest number of positive reviews\n",
        "positive_df=df2[df2.Sentiment=='Positive']\n",
        "positive_df.groupby('App').Sentiment.value_counts().nlargest(20).plot.barh(figsize=(15,6),color='violet').invert_yaxis()\n",
        "plt.xlabel('Total number of positive reviews')\n",
        "plt.title('Apps with the highest number of positive reviews')"
      ],
      "metadata": {
        "id": "t3ZrW70o-QVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apps with the highest number of negative reviews\n",
        "negative_df=df2[df2.Sentiment=='Negative']\n",
        "negative_df.groupby('App').Sentiment.value_counts().nlargest(20).plot.barh(figsize=(15,6),color='purple').invert_yaxis()\n",
        "plt.xlabel('Total number of negative reviews')\n",
        "plt.title('Apps with the highest number of negative reviews')"
      ],
      "metadata": {
        "id": "aWHl0GL8-Xy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Is sentiment_subjectivity proportional to sentiment_polarity?\n",
        "# scatterplot of sentiment polarity and sentiment subjectivity\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.scatterplot(x=df2['Sentiment_Subjectivity'], y=df2['Sentiment_Polarity'],\n",
        "                hue = df2['Sentiment'], edgecolor='white')\n",
        "plt.title(\"Google Play Store Reviews Sentiment Analysis\", fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tPUpkguK-izP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "r5urZI5Swv3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we pick the above graph to know percentage of all type of reviews,Apps with the highest number of positive reviews and negative reviews and checking Is sentiment_subjectivity proportional to sentiment_polarity or not."
      ],
      "metadata": {
        "id": "U98l24xywv3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "vMG9UoRUwv3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from th avobe graph\n",
        "\n",
        "64.30% of reviews are positive.\n",
        "\n",
        "22.80% of reviews are negative\n",
        "\n",
        "12.90% of reviews are neutral.\n",
        "\n",
        "It is clear that Angry Birds Classic has got highest negative reviews and It is evident that Helix Jump has received the most positive reviews.\n",
        "\n",
        "In the scatter figure above shows that sentiment subjectivity is most often related to sentiment polarity when variance is too high or too low, although that is not always the case."
      ],
      "metadata": {
        "id": "erp2KtSlwv3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "4-1mLqvtwv3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "JY-XAIk2wv3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "s2SN7JSOwv3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Finding correlation between different columns in the play store data\n",
        "df1.corr()"
      ],
      "metadata": {
        "id": "DbrRldmnwv3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heat map for playstore_df\n",
        "plt.figure(figsize = (15,9))\n",
        "sns.heatmap(df1.corr(), annot= True,cmap='Greens')\n",
        "plt.title('Corelation Heatmap for Playstore Data', size=25)"
      ],
      "metadata": {
        "id": "_1nX7UvS_VUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging 2 data frame\n",
        "merge_df = pd.merge(df1, df2, on='App', how = \"inner\")\n",
        "merge_df.shape"
      ],
      "metadata": {
        "id": "i6ZWJGYD_cgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_df.info()"
      ],
      "metadata": {
        "id": "3QLT4GwD_jLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding correlation between different columns in the play store data\n",
        "merge_df.corr()"
      ],
      "metadata": {
        "id": "LKY396US_ny-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heat map for merge_df\n",
        "plt.figure(figsize = (15,9))\n",
        "sns.heatmap(merge_df.corr(), annot= True,cmap='Blues')\n",
        "plt.title('Corelation Heatmap for Playstore Data', size=25)"
      ],
      "metadata": {
        "id": "hFSfD8AU_yK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gqHSq_Rjwv3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we pick this chat to know the corelation between each column of playstore dataset and also the corelation between both the dataset."
      ],
      "metadata": {
        "id": "PShtZnGHwv3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "wwK00qxxwv3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The Reviews and Installs columns have a significant positive association. This is essentially clear. The number of installs, user base, and overall reviews dropped by users all increase as the number of installs increases.\n",
        "\n",
        "2.The Price has a weakly inverse relationship to the Rating, Reviews, and Installs. This translates to a minor decline in the average rating, overall number of reviews, and installs when the app's price rises.\n",
        "\n",
        "3.The Installs and Reviews columns have a slightly positive correlation with the Rating. This suggests that as the average user rating rises, so do the number of app installs and reviews."
      ],
      "metadata": {
        "id": "7b91spWswv3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "yiJy0Q77wv3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Pair Plot "
      ],
      "metadata": {
        "id": "_8HsLGV5wv3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "Rating = df1['Rating']\n",
        "Size = df1['Size']\n",
        "Installs = df1['Installs']\n",
        "Reviews = df1['Reviews']\n",
        "Type = df1['Type']\n",
        "Price = df1['Price']\n",
        "\n",
        "p = sns.pairplot(pd.DataFrame(list(zip(Rating, np.log(Installs), np.log10(Reviews), Price, Type)), \n",
        "                        columns=['Rating', 'Installs', 'Reviews', 'Price','Type']), hue='Type')\n",
        "p.fig.suptitle(\"Pairwise Plot - Rating,  Installs, Reviews, Price\",x=0.5, y=1.0, fontsize=16)"
      ],
      "metadata": {
        "id": "Ezjazf-xwv3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVqNjjZBBZGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "_-mdh9CYwv3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plots are used to determine the most distinct clusters or the best combination of features to describe a connection between two variables. By creating some straightforward linear separations or basic lines in our data set, it also helps to create some straightforward classification models.\n",
        "\n",
        "Draw a pairwise plot between each quantitative variable to search for any patterns or connections between the features that stand out."
      ],
      "metadata": {
        "id": "eZW2a7H_wv3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "74dCLe9xwv3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Most apps are free to download.\n",
        "\n",
        "2 The majority of paid apps have an average rating of 4.\n",
        "\n",
        "3 As the number of installations rises, so do the reviews for each individual app."
      ],
      "metadata": {
        "id": "guXk6ntYwv3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Solution to Business Objective**"
      ],
      "metadata": {
        "id": "2pQ4Wk4Ew6oM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What do you suggest the client to achieve Business Objective ? \n",
        "Explain Briefly."
      ],
      "metadata": {
        "id": "1QVImLEww6oN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Summary**\n",
        "\n",
        "We have worked on a number of metrics as part of this study to analyse play store applications that would helps the client launch their apps successfully.\n",
        "\n",
        "\n",
        "In order to give them the best results from our study, we concentrated more on the issue descriptions and data cleaning in the initial phase.\n",
        "\n",
        "The Client has to concentrate more on: \n",
        "\n",
        "* Creating applications for the least-explored categories. like things like beauty.\n",
        "\n",
        "* As the majority of applications are free, emphasising free apps is more crucial.\n",
        "\n",
        "* In order to achieve the largest install rates, you should concentrate more on content that is accessible to everyone.\n",
        "\n",
        "* Companies should concentrate on continuously updating their apps to attract new users.\n",
        "\n",
        "* They should pay greater attention to the wants and features of the user because the user's feelings will change as they use the app.\n",
        "\n",
        "* The most competitive category is family, while the category with the highest average app installs is games. The percentage of free apps is 92%, and the percentage of no age-restricted apps is 82%.\n",
        "\n",
        "* With 1906, 926, and 829 apps respectively, the top three categories are family, games, and tools.\n",
        "\n",
        "* The leading genres are tools, entertainment, education, business, and medicine.\n",
        "\n",
        "* There are 8783 apps that are under 50 MB in size. Both types of apps are included in the 7749 apps with ratings greater than 4.0.\n",
        "\n",
        "* Almost a billion people have downloaded a specipic 20 free applications.\n",
        "\n",
        "* The only paid app with more than 10M installations is Minecraft. Also, this app has generated the most money only from the installation price. The category with the highest average installation cost for paid apps is: lifestyle\n",
        "\n",
        "* The average app size in the Google Play store is 12 MB. The apps with the highest average number of installs are those whose size varies by device.\n",
        "\n",
        "* The largest number of average user ratings are found in apps larger than 90 MB, indicating that they are much more popular than the others.\n",
        "\n",
        "* The game with the most good reviews is Helix Jump, whereas the game with the most negative reviews is Angry Birds Classic.\n",
        "\n",
        "* The combined dataset's overall sentiment score is 64% positive, 22% negative, and 13% neutral."
      ],
      "metadata": {
        "id": "Y2z0OiGvw6oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "5vbiWx47xFKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First of all Data cleaning was one of our biggest challenges.\n",
        "\n",
        "2. 13.60% of the reviews had NaN values, and even after combining the two dataframes, we were unable to make any significant deductions to fill them. So we had to abandon them.\n",
        "\n",
        "3. There were only 816 common apps in the combined data frame from the Play Store and user evaluations. We could have provided a more insightful analysis if we had at least 70% to 80% of the cleaned data available in the merged dataframes, which is only 10% of the total data.\n",
        "\n",
        "4. The User Reviews column had 13.60% NaN values, which may have been filled by using the 42% NaN values to better grasp the attitudes within each category.\n",
        "\n",
        "5. There is so much more to learn about. As an example, we have the current version and the Android version available, which can be investigated in greater detail and lead to additional analysis where we can explain how these things affect and need to be considered when designing apps for users.\n",
        "\n",
        "6. We can investigate the impact of the app's size and Android version on the quantity of installs.\n",
        "\n",
        "7. By creating models that can help us interpret even better, machine learning can assist us in deploying more insights. Although this is something we can work on, we have left it for later."
      ],
      "metadata": {
        "id": "-QXMK1aUxFKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your EDA Capstone Project !!!***"
      ],
      "metadata": {
        "id": "XArpCiv7xFKQ"
      }
    }
  ]
}